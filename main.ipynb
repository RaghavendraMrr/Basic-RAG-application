{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef5d4940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "419394a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing\n"
     ]
    }
   ],
   "source": [
    "print(\"testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b205f026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a6cc016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python versin is:3.11.2 (tags/v3.11.2:878ead1, Feb  7 2023, 16:38:35) [MSC v.1934 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"python versin is:{sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "765383d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the version of python is 3.11.2 (tags/v3.11.2:878ead1, Feb  7 2023, 16:38:35) [MSC v.1934 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"the version of python is {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db74e3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version is comptabile\n"
     ]
    }
   ],
   "source": [
    "if sys.version_info>=(3,9):\n",
    "    print(\"python version is comptabile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8029ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Package(s) not found: langchain\n"
     ]
    }
   ],
   "source": [
    "pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf56594a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in e:\\genai\\samplerag\\lib\\site-packages (from -r requirements.txt (line 9)) (1.0.7)\n",
      "Requirement already satisfied: langchain-core in e:\\genai\\samplerag\\lib\\site-packages (from -r requirements.txt (line 10)) (1.0.5)\n",
      "Collecting langchain-community\n",
      "  Using cached langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
      "Collecting langchain-text-splitters\n",
      "  Using cached langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\n",
      "Collecting langchain-openai\n",
      "  Using cached langchain_openai-1.0.3-py3-none-any.whl (82 kB)\n",
      "Collecting openai\n",
      "  Using cached openai-2.8.1-py3-none-any.whl (1.0 MB)\n",
      "Collecting langchain-google-genai\n",
      "  Using cached langchain_google_genai-3.0.3-py3-none-any.whl (56 kB)\n",
      "Collecting google-generativeai\n",
      "  Using cached google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n",
      "Collecting langchain-huggingface\n",
      "  Using cached langchain_huggingface-1.0.1-py3-none-any.whl (27 kB)\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-5.1.2-py3-none-any.whl (488 kB)\n",
      "Collecting faiss-cpu\n",
      "  Using cached faiss_cpu-1.13.0-cp311-cp311-win_amd64.whl (18.7 MB)\n",
      "Collecting langchain-chroma\n",
      "  Using cached langchain_chroma-1.0.0-py3-none-any.whl (12 kB)\n",
      "Collecting chromadb\n",
      "  Using cached chromadb-1.3.4-cp39-abi3-win_amd64.whl (20.8 MB)\n",
      "Collecting pypdf\n",
      "  Using cached pypdf-6.3.0-py3-none-any.whl (328 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.14.2-py3-none-any.whl (106 kB)\n",
      "     -------------------------------------- 106.4/106.4 kB 1.2 MB/s eta 0:00:00\n",
      "Collecting lxml\n",
      "  Using cached lxml-6.0.2-cp311-cp311-win_amd64.whl (4.0 MB)\n",
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Collecting tiktoken\n",
      "  Using cached tiktoken-0.12.0-cp311-cp311-win_amd64.whl (879 kB)\n",
      "Collecting jupyter\n",
      "  Using cached jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
      "Collecting notebook\n",
      "  Using cached notebook-7.4.7-py3-none-any.whl (14.3 MB)\n",
      "Requirement already satisfied: ipykernel in e:\\genai\\samplerag\\lib\\site-packages (from -r requirements.txt (line 75)) (7.1.0)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.3.5-cp311-cp311-win_amd64.whl (13.1 MB)\n",
      "     ---------------------------------------- 13.1/13.1 MB 4.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in e:\\genai\\samplerag\\lib\\site-packages (from langchain->-r requirements.txt (line 9)) (1.0.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in e:\\genai\\samplerag\\lib\\site-packages (from langchain->-r requirements.txt (line 9)) (2.12.4)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in e:\\genai\\samplerag\\lib\\site-packages (from langchain-core->-r requirements.txt (line 10)) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in e:\\genai\\samplerag\\lib\\site-packages (from langchain-core->-r requirements.txt (line 10)) (0.4.43)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in e:\\genai\\samplerag\\lib\\site-packages (from langchain-core->-r requirements.txt (line 10)) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in e:\\genai\\samplerag\\lib\\site-packages (from langchain-core->-r requirements.txt (line 10)) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in e:\\genai\\samplerag\\lib\\site-packages (from langchain-core->-r requirements.txt (line 10)) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in e:\\genai\\samplerag\\lib\\site-packages (from langchain-core->-r requirements.txt (line 10)) (4.15.0)\n",
      "Collecting langchain-classic<2.0.0,>=1.0.0\n",
      "  Using cached langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n",
      "Collecting SQLAlchemy<3.0.0,>=1.4.0\n",
      "  Using cached sqlalchemy-2.0.44-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.5 in e:\\genai\\samplerag\\lib\\site-packages (from langchain-community->-r requirements.txt (line 11)) (2.32.5)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3\n",
      "  Using cached aiohttp-3.13.2-cp311-cp311-win_amd64.whl (456 kB)\n",
      "Collecting dataclasses-json<0.7.0,>=0.6.7\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.10.1\n",
      "  Using cached pydantic_settings-2.12.0-py3-none-any.whl (51 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0\n",
      "  Using cached httpx_sse-0.4.3-py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in e:\\genai\\samplerag\\lib\\site-packages (from openai->-r requirements.txt (line 19)) (4.11.0)\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in e:\\genai\\samplerag\\lib\\site-packages (from openai->-r requirements.txt (line 19)) (0.28.1)\n",
      "Collecting jiter<1,>=0.10.0\n",
      "  Using cached jiter-0.12.0-cp311-cp311-win_amd64.whl (204 kB)\n",
      "Requirement already satisfied: sniffio in e:\\genai\\samplerag\\lib\\site-packages (from openai->-r requirements.txt (line 19)) (1.3.1)\n",
      "Collecting tqdm>4\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Collecting google-ai-generativelanguage<1.0.0,>=0.7.0\n",
      "  Using cached google_ai_generativelanguage-0.9.0-py3-none-any.whl (1.4 MB)\n",
      "Collecting google-generativeai\n",
      "  Using cached google_generativeai-0.8.4-py3-none-any.whl (175 kB)\n",
      "  Using cached google_generativeai-0.8.3-py3-none-any.whl (160 kB)\n",
      "  Using cached google_generativeai-0.8.2-py3-none-any.whl (153 kB)\n",
      "  Using cached google_generativeai-0.8.1-py3-none-any.whl (165 kB)\n",
      "  Using cached google_generativeai-0.8.0-py3-none-any.whl (163 kB)\n",
      "  Using cached google_generativeai-0.7.2-py3-none-any.whl (164 kB)\n",
      "  Using cached google_generativeai-0.7.1-py3-none-any.whl (163 kB)\n",
      "  Using cached google_generativeai-0.7.0-py3-none-any.whl (163 kB)\n",
      "  Using cached google_generativeai-0.6.0-py3-none-any.whl (158 kB)\n",
      "  Using cached google_generativeai-0.5.4-py3-none-any.whl (150 kB)\n",
      "  Using cached google_generativeai-0.5.3-py3-none-any.whl (150 kB)\n",
      "  Using cached google_generativeai-0.5.2-py3-none-any.whl (146 kB)\n",
      "  Using cached google_generativeai-0.5.1-py3-none-any.whl (142 kB)\n",
      "  Using cached google_generativeai-0.5.0-py3-none-any.whl (142 kB)\n",
      "  Using cached google_generativeai-0.4.1-py3-none-any.whl (137 kB)\n",
      "  Using cached google_generativeai-0.4.0-py3-none-any.whl (137 kB)\n",
      "  Using cached google_generativeai-0.3.2-py3-none-any.whl (146 kB)\n",
      "  Using cached google_generativeai-0.3.1-py3-none-any.whl (146 kB)\n",
      "  Using cached google_generativeai-0.3.0-py3-none-any.whl (145 kB)\n",
      "  Using cached google_generativeai-0.2.2-py3-none-any.whl (133 kB)\n",
      "  Using cached google_generativeai-0.2.1-py3-none-any.whl (130 kB)\n",
      "  Using cached google_generativeai-0.2.0-py3-none-any.whl (130 kB)\n",
      "  Using cached google_generativeai-0.1.0-py3-none-any.whl (122 kB)\n",
      "INFO: pip is looking at multiple versions of langchain-google-genai to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-google-genai\n",
      "  Using cached langchain_google_genai-3.0.2-py3-none-any.whl (55 kB)\n",
      "  Using cached langchain_google_genai-3.0.1-py3-none-any.whl (58 kB)\n",
      "  Using cached langchain_google_genai-3.0.0-py3-none-any.whl (57 kB)\n",
      "  Using cached langchain_google_genai-2.1.12-py3-none-any.whl (50 kB)\n",
      "  Using cached langchain_google_genai-2.1.11-py3-none-any.whl (50 kB)\n",
      "  Using cached langchain_google_genai-2.1.10-py3-none-any.whl (49 kB)\n",
      "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18\n",
      "  Using cached google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
      "Collecting langchain-google-genai\n",
      "  Using cached langchain_google_genai-2.1.9-py3-none-any.whl (49 kB)\n",
      "  Using cached langchain_google_genai-2.1.8-py3-none-any.whl (47 kB)\n",
      "  Using cached langchain_google_genai-2.1.7-py3-none-any.whl (47 kB)\n",
      "  Using cached langchain_google_genai-2.1.6-py3-none-any.whl (47 kB)\n",
      "  Using cached langchain_google_genai-2.1.5-py3-none-any.whl (44 kB)\n",
      "  Using cached langchain_google_genai-2.1.4-py3-none-any.whl (44 kB)\n",
      "  Using cached langchain_google_genai-2.1.3-py3-none-any.whl (43 kB)\n",
      "  Using cached langchain_google_genai-2.1.2-py3-none-any.whl (42 kB)\n",
      "  Using cached langchain_google_genai-2.1.1-py3-none-any.whl (40 kB)\n",
      "  Using cached langchain_google_genai-2.1.0-py3-none-any.whl (40 kB)\n",
      "  Using cached langchain_google_genai-2.0.11-py3-none-any.whl (39 kB)\n",
      "  Using cached langchain_google_genai-2.0.10-py3-none-any.whl (41 kB)\n",
      "  Using cached langchain_google_genai-2.0.9-py3-none-any.whl (41 kB)\n",
      "  Using cached langchain_google_genai-2.0.8-py3-none-any.whl (41 kB)\n",
      "  Using cached langchain_google_genai-2.0.7-py3-none-any.whl (41 kB)\n",
      "  Using cached langchain_google_genai-2.0.6-py3-none-any.whl (41 kB)\n",
      "  Using cached langchain_google_genai-2.0.5-py3-none-any.whl (41 kB)\n",
      "  Using cached langchain_google_genai-2.0.4-py3-none-any.whl (41 kB)\n",
      "  Using cached langchain_google_genai-2.0.3-py3-none-any.whl (42 kB)\n",
      "  Using cached langchain_google_genai-2.0.2-py3-none-any.whl (42 kB)\n",
      "  Using cached langchain_google_genai-2.0.1-py3-none-any.whl (40 kB)\n",
      "  Using cached langchain_google_genai-2.0.0-py3-none-any.whl (39 kB)\n",
      "  Using cached langchain_google_genai-1.0.10-py3-none-any.whl (39 kB)\n",
      "  Using cached langchain_google_genai-1.0.9-py3-none-any.whl (39 kB)\n",
      "  Using cached langchain_google_genai-1.0.8-py3-none-any.whl (38 kB)\n",
      "  Using cached langchain_google_genai-1.0.7-py3-none-any.whl (36 kB)\n",
      "  Using cached langchain_google_genai-1.0.6-py3-none-any.whl (35 kB)\n",
      "  Using cached langchain_google_genai-1.0.5-py3-none-any.whl (34 kB)\n",
      "  Using cached langchain_google_genai-1.0.4-py3-none-any.whl (33 kB)\n",
      "  Using cached langchain_google_genai-1.0.3-py3-none-any.whl (31 kB)\n",
      "  Using cached langchain_google_genai-1.0.2-py3-none-any.whl (28 kB)\n",
      "  Using cached langchain_google_genai-1.0.1-py3-none-any.whl (28 kB)\n",
      "  Using cached langchain_google_genai-0.0.11-py3-none-any.whl (28 kB)\n",
      "  Using cached langchain_google_genai-0.0.9-py3-none-any.whl (17 kB)\n",
      "  Using cached langchain_google_genai-0.0.8-py3-none-any.whl (16 kB)\n",
      "  Using cached langchain_google_genai-0.0.7-py3-none-any.whl (16 kB)\n",
      "  Using cached langchain_google_genai-0.0.6-py3-none-any.whl (15 kB)\n",
      "  Using cached langchain_google_genai-0.0.5-py3-none-any.whl (14 kB)\n",
      "  Using cached langchain_google_genai-0.0.4-py3-none-any.whl (11 kB)\n",
      "  Using cached langchain_google_genai-0.0.3-py3-none-any.whl (11 kB)\n",
      "  Using cached langchain_google_genai-0.0.2-py3-none-any.whl (8.8 kB)\n",
      "  Using cached langchain_google_genai-0.0.1-py3-none-any.whl (8.5 kB)\n",
      "Collecting google-ai-generativelanguage==0.4.0\n",
      "  Using cached google_ai_generativelanguage-0.4.0-py3-none-any.whl (598 kB)\n",
      "Collecting google-auth\n",
      "  Using cached google_auth-2.43.0-py2.py3-none-any.whl (223 kB)\n",
      "Collecting google-api-core\n",
      "  Using cached google_api_core-2.28.1-py3-none-any.whl (173 kB)\n",
      "Collecting protobuf\n",
      "  Using cached protobuf-6.33.1-cp310-abi3-win_amd64.whl (436 kB)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3\n",
      "  Using cached proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Collecting protobuf\n",
      "  Using cached protobuf-4.25.8-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Collecting huggingface-hub<1.0.0,>=0.33.4\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "Collecting tokenizers<1.0.0,>=0.19.1\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "Collecting transformers<5.0.0,>=4.41.0\n",
      "  Using cached transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "Collecting torch>=1.11.0\n",
      "  Using cached torch-2.9.1-cp311-cp311-win_amd64.whl (111.0 MB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.2-cp311-cp311-win_amd64.whl (8.9 MB)\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.16.3-cp311-cp311-win_amd64.whl (38.7 MB)\n",
      "Collecting Pillow\n",
      "  Using cached pillow-12.0.0-cp311-cp311-win_amd64.whl (7.0 MB)\n",
      "Collecting build>=1.0.3\n",
      "  Using cached build-1.3.0-py3-none-any.whl (23 kB)\n",
      "Collecting pybase64>=1.4.1\n",
      "  Using cached pybase64-1.4.2-cp311-cp311-win_amd64.whl (35 kB)\n",
      "Collecting uvicorn[standard]>=0.18.3\n",
      "  Using cached uvicorn-0.38.0-py3-none-any.whl (68 kB)\n",
      "Collecting posthog<6.0.0,>=2.4.0\n",
      "  Using cached posthog-5.4.0-py3-none-any.whl (105 kB)\n",
      "Collecting onnxruntime>=1.14.1\n",
      "  Using cached onnxruntime-1.23.2-cp311-cp311-win_amd64.whl (13.5 MB)\n",
      "Collecting opentelemetry-api>=1.2.0\n",
      "  Using cached opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl (19 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0\n",
      "  Using cached opentelemetry_sdk-1.38.0-py3-none-any.whl (132 kB)\n",
      "Collecting pypika>=0.48.9\n",
      "  Using cached pypika-0.48.9-py2.py3-none-any.whl\n",
      "Collecting overrides>=7.3.1\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Collecting importlib-resources\n",
      "  Using cached importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Collecting grpcio>=1.58.0\n",
      "  Using cached grpcio-1.76.0-cp311-cp311-win_amd64.whl (4.7 MB)\n",
      "Collecting bcrypt>=4.0.1\n",
      "  Using cached bcrypt-5.0.0-cp39-abi3-win_amd64.whl (150 kB)\n",
      "Collecting typer>=0.9.0\n",
      "  Using cached typer-0.20.0-py3-none-any.whl (47 kB)\n",
      "Collecting kubernetes>=28.1.0\n",
      "  Using cached kubernetes-34.1.0-py2.py3-none-any.whl (2.0 MB)\n",
      "Collecting mmh3>=4.0.1\n",
      "  Using cached mmh3-5.2.0-cp311-cp311-win_amd64.whl (41 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in e:\\genai\\samplerag\\lib\\site-packages (from chromadb->-r requirements.txt (line 40)) (3.11.4)\n",
      "Collecting rich>=10.11.0\n",
      "  Using cached rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Collecting jsonschema>=4.19.0\n",
      "  Using cached jsonschema-4.25.1-py3-none-any.whl (90 kB)\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.8-py3-none-any.whl (36 kB)\n",
      "Collecting regex>=2022.1.18\n",
      "  Downloading regex-2025.11.3-cp311-cp311-win_amd64.whl (277 kB)\n",
      "     -------------------------------------- 277.7/277.7 kB 1.9 MB/s eta 0:00:00\n",
      "Collecting jupyter-console\n",
      "  Using cached jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n",
      "Collecting nbconvert\n",
      "  Using cached nbconvert-7.16.6-py3-none-any.whl (258 kB)\n",
      "Collecting ipywidgets\n",
      "  Using cached ipywidgets-8.1.8-py3-none-any.whl (139 kB)\n",
      "Collecting jupyterlab\n",
      "  Using cached jupyterlab-4.4.10-py3-none-any.whl (12.3 MB)\n",
      "Collecting jupyter-server<3,>=2.4.0\n",
      "  Using cached jupyter_server-2.17.0-py3-none-any.whl (388 kB)\n",
      "Collecting jupyterlab-server<3,>=2.27.1\n",
      "  Using cached jupyterlab_server-2.28.0-py3-none-any.whl (59 kB)\n",
      "Collecting notebook-shim<0.3,>=0.2\n",
      "  Using cached notebook_shim-0.2.4-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: tornado>=6.2.0 in e:\\genai\\samplerag\\lib\\site-packages (from notebook->-r requirements.txt (line 74)) (6.5.2)\n",
      "Requirement already satisfied: comm>=0.1.1 in e:\\genai\\samplerag\\lib\\site-packages (from ipykernel->-r requirements.txt (line 75)) (0.2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in e:\\genai\\samplerag\\lib\\site-packages (from ipykernel->-r requirements.txt (line 75)) (1.8.17)\n",
      "Requirement already satisfied: ipython>=7.23.1 in e:\\genai\\samplerag\\lib\\site-packages (from ipykernel->-r requirements.txt (line 75)) (9.7.0)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in e:\\genai\\samplerag\\lib\\site-packages (from ipykernel->-r requirements.txt (line 75)) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in e:\\genai\\samplerag\\lib\\site-packages (from ipykernel->-r requirements.txt (line 75)) (5.9.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in e:\\genai\\samplerag\\lib\\site-packages (from ipykernel->-r requirements.txt (line 75)) (0.2.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in e:\\genai\\samplerag\\lib\\site-packages (from ipykernel->-r requirements.txt (line 75)) (1.6.0)\n",
      "Requirement already satisfied: psutil>=5.7 in e:\\genai\\samplerag\\lib\\site-packages (from ipykernel->-r requirements.txt (line 75)) (7.1.3)\n",
      "Requirement already satisfied: pyzmq>=25 in e:\\genai\\samplerag\\lib\\site-packages (from ipykernel->-r requirements.txt (line 75)) (27.1.0)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in e:\\genai\\samplerag\\lib\\site-packages (from ipykernel->-r requirements.txt (line 75)) (5.14.3)\n",
      "Collecting aiohappyeyeballs>=2.5.0\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting aiosignal>=1.4.0\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.8.0-cp311-cp311-win_amd64.whl (44 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.7.0-cp311-cp311-win_amd64.whl (46 kB)\n",
      "Collecting propcache>=0.2.0\n",
      "  Using cached propcache-0.4.1-cp311-cp311-win_amd64.whl (41 kB)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Using cached yarl-1.22.0-cp311-cp311-win_amd64.whl (86 kB)\n",
      "Requirement already satisfied: idna>=2.8 in e:\\genai\\samplerag\\lib\\site-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 19)) (3.11)\n",
      "Collecting pyproject_hooks\n",
      "  Using cached pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: colorama in e:\\genai\\samplerag\\lib\\site-packages (from build>=1.0.3->chromadb->-r requirements.txt (line 40)) (0.4.6)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: certifi in e:\\genai\\samplerag\\lib\\site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 19)) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in e:\\genai\\samplerag\\lib\\site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 19)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in e:\\genai\\samplerag\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 19)) (0.16.0)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "     -------------------------------------- 201.0/201.0 kB 6.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: decorator>=4.3.2 in e:\\genai\\samplerag\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 75)) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in e:\\genai\\samplerag\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 75)) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.18.1 in e:\\genai\\samplerag\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 75)) (0.19.2)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in e:\\genai\\samplerag\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 75)) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.11.0 in e:\\genai\\samplerag\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 75)) (2.19.2)\n",
      "Requirement already satisfied: stack_data>=0.6.0 in e:\\genai\\samplerag\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 75)) (0.6.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in e:\\genai\\samplerag\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core->-r requirements.txt (line 10)) (3.0.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6\n",
      "  Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)\n",
      "Collecting referencing>=0.28.4\n",
      "  Using cached referencing-0.37.0-py3-none-any.whl (26 kB)\n",
      "Collecting rpds-py>=0.7.1\n",
      "  Downloading rpds_py-0.29.0-cp311-cp311-win_amd64.whl (235 kB)\n",
      "     -------------------------------------- 235.7/235.7 kB 4.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\genai\\samplerag\\lib\\site-packages (from jupyter-client>=8.0.0->ipykernel->-r requirements.txt (line 75)) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in e:\\genai\\samplerag\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->-r requirements.txt (line 75)) (4.5.0)\n",
      "Collecting argon2-cffi>=21.1\n",
      "  Using cached argon2_cffi-25.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting jinja2>=3.0.3\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Collecting jupyter-events>=0.11.0\n",
      "  Using cached jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
      "Collecting jupyter-server-terminals>=0.4.4\n",
      "  Using cached jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
      "Collecting nbformat>=5.3.0\n",
      "  Using cached nbformat-5.10.4-py3-none-any.whl (78 kB)\n",
      "Collecting prometheus-client>=0.9\n",
      "  Using cached prometheus_client-0.23.1-py3-none-any.whl (61 kB)\n",
      "Collecting pywinpty>=2.0.1\n",
      "  Using cached pywinpty-3.0.2-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "Collecting send2trash>=1.8.2\n",
      "  Using cached Send2Trash-1.8.3-py3-none-any.whl (18 kB)\n",
      "Collecting terminado>=0.8.3\n",
      "  Using cached terminado-0.18.1-py3-none-any.whl (14 kB)\n",
      "Collecting websocket-client>=1.7\n",
      "  Using cached websocket_client-1.9.0-py3-none-any.whl (82 kB)\n",
      "Collecting async-lru>=1.0.0\n",
      "  Using cached async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
      "Collecting jupyter-lsp>=2.0.0\n",
      "  Using cached jupyter_lsp-2.3.0-py3-none-any.whl (76 kB)\n",
      "Requirement already satisfied: setuptools>=41.1.0 in e:\\genai\\samplerag\\lib\\site-packages (from jupyterlab->jupyter->-r requirements.txt (line 73)) (65.5.0)\n",
      "Collecting babel>=2.10\n",
      "  Using cached babel-2.17.0-py3-none-any.whl (10.2 MB)\n",
      "Collecting json5>=0.9.0\n",
      "  Using cached json5-0.12.1-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in e:\\genai\\samplerag\\lib\\site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 40)) (1.17.0)\n",
      "Collecting requests-oauthlib\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting urllib3<2.4.0,>=1.24.2\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "     -------------------------------------- 128.4/128.4 kB 7.9 MB/s eta 0:00:00\n",
      "Collecting durationpy>=0.7\n",
      "  Using cached durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
      "Collecting cachetools<7.0,>=2.0.0\n",
      "  Using cached cachetools-6.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in e:\\genai\\samplerag\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain->-r requirements.txt (line 9)) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in e:\\genai\\samplerag\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain->-r requirements.txt (line 9)) (1.0.4)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in e:\\genai\\samplerag\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain->-r requirements.txt (line 9)) (0.2.9)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in e:\\genai\\samplerag\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain->-r requirements.txt (line 9)) (3.6.0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in e:\\genai\\samplerag\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core->-r requirements.txt (line 10)) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in e:\\genai\\samplerag\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core->-r requirements.txt (line 10)) (0.25.0)\n",
      "Collecting bleach[css]!=5.0.0\n",
      "  Using cached bleach-6.3.0-py3-none-any.whl (164 kB)\n",
      "Collecting defusedxml\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Collecting jupyterlab-pygments\n",
      "  Using cached jupyterlab_pygments-0.3.0-py3-none-any.whl (15 kB)\n",
      "Collecting markupsafe>=2.0\n",
      "  Using cached markupsafe-3.0.3-cp311-cp311-win_amd64.whl (15 kB)\n",
      "Collecting mistune<4,>=2.0.3\n",
      "  Using cached mistune-3.1.4-py3-none-any.whl (53 kB)\n",
      "Collecting nbclient>=0.5.0\n",
      "  Using cached nbclient-0.10.2-py3-none-any.whl (25 kB)\n",
      "Collecting pandocfilters>=1.4.1\n",
      "  Using cached pandocfilters-1.5.1-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting coloredlogs\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Collecting flatbuffers\n",
      "  Using cached flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Collecting sympy\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Collecting importlib-metadata<8.8.0,>=6.0\n",
      "  Using cached importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Collecting googleapis-common-protos~=1.57\n",
      "  Using cached googleapis_common_protos-1.72.0-py3-none-any.whl (297 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.38.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-proto==1.38.0\n",
      "  Using cached opentelemetry_proto-1.38.0-py3-none-any.whl (72 kB)\n",
      "INFO: pip is looking at multiple versions of opentelemetry-exporter-otlp-proto-common to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of opentelemetry-exporter-otlp-proto-grpc to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.37.0-py3-none-any.whl (19 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.37.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.37.0-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-proto==1.37.0\n",
      "  Using cached opentelemetry_proto-1.37.0-py3-none-any.whl (72 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0\n",
      "  Using cached opentelemetry_sdk-1.37.0-py3-none-any.whl (131 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.36.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-proto==1.36.0\n",
      "  Using cached opentelemetry_proto-1.36.0-py3-none-any.whl (72 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0\n",
      "  Using cached opentelemetry_sdk-1.36.0-py3-none-any.whl (119 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.35.0-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.35.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.35.0-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-proto==1.35.0\n",
      "  Using cached opentelemetry_proto-1.35.0-py3-none-any.whl (72 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0\n",
      "  Using cached opentelemetry_sdk-1.35.0-py3-none-any.whl (119 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.34.1\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-proto==1.34.1\n",
      "  Using cached opentelemetry_proto-1.34.1-py3-none-any.whl (55 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0\n",
      "  Using cached opentelemetry_sdk-1.34.1-py3-none-any.whl (118 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.34.0-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.34.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.34.0-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-proto==1.34.0\n",
      "  Using cached opentelemetry_proto-1.34.0-py3-none-any.whl (55 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.33.1-py3-none-any.whl (18 kB)\n",
      "Collecting deprecated>=1.2.6\n",
      "  Using cached deprecated-1.3.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.33.1\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.33.1-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-proto==1.33.1\n",
      "  Using cached opentelemetry_proto-1.33.1-py3-none-any.whl (55 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0\n",
      "  Using cached opentelemetry_sdk-1.33.1-py3-none-any.whl (118 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.33.0-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.33.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.33.0-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-proto==1.33.0\n",
      "  Using cached opentelemetry_proto-1.33.0-py3-none-any.whl (55 kB)\n",
      "INFO: pip is looking at multiple versions of opentelemetry-exporter-otlp-proto-grpc to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.32.1-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.32.1\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.32.1-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-proto==1.32.1\n",
      "  Using cached opentelemetry_proto-1.32.1-py3-none-any.whl (55 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0\n",
      "  Using cached opentelemetry_sdk-1.32.1-py3-none-any.whl (118 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.32.0-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.32.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.32.0-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-proto==1.32.0\n",
      "  Using cached opentelemetry_proto-1.32.0-py3-none-any.whl (55 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.31.1-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.31.1\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.31.1-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-proto==1.31.1\n",
      "  Using cached opentelemetry_proto-1.31.1-py3-none-any.whl (55 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0\n",
      "  Using cached opentelemetry_sdk-1.31.1-py3-none-any.whl (118 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.31.0-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.31.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.31.0-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-proto==1.31.0\n",
      "  Using cached opentelemetry_proto-1.31.0-py3-none-any.whl (55 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.30.0-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.30.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.30.0-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-proto==1.30.0\n",
      "  Using cached opentelemetry_proto-1.30.0-py3-none-any.whl (55 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0\n",
      "  Using cached opentelemetry_sdk-1.30.0-py3-none-any.whl (118 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.29.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-proto==1.29.0\n",
      "  Using cached opentelemetry_proto-1.29.0-py3-none-any.whl (55 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0\n",
      "  Using cached opentelemetry_sdk-1.29.0-py3-none-any.whl (118 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.28.2-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.28.2\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.28.2-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-proto==1.28.2\n",
      "  Using cached opentelemetry_proto-1.28.2-py3-none-any.whl (55 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0\n",
      "  Using cached opentelemetry_sdk-1.28.2-py3-none-any.whl (118 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.28.1-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.28.1\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.28.1-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-proto==1.28.1\n",
      "  Using cached opentelemetry_proto-1.28.1-py3-none-any.whl (55 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.28.0-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.28.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.28.0-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-proto==1.28.0\n",
      "  Using cached opentelemetry_proto-1.28.0-py3-none-any.whl (55 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.27.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl (17 kB)\n",
      "Collecting opentelemetry-proto==1.27.0\n",
      "  Using cached opentelemetry_proto-1.27.0-py3-none-any.whl (52 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0\n",
      "  Using cached opentelemetry_sdk-1.27.0-py3-none-any.whl (110 kB)\n",
      "Collecting opentelemetry-api>=1.2.0\n",
      "  Using cached opentelemetry_api-1.27.0-py3-none-any.whl (63 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.48b0\n",
      "  Using cached opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl (149 kB)\n",
      "Collecting importlib-metadata<8.8.0,>=6.0\n",
      "  Using cached importlib_metadata-8.4.0-py3-none-any.whl (26 kB)\n",
      "Collecting backoff>=1.10.0\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in e:\\genai\\samplerag\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirements.txt (line 9)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in e:\\genai\\samplerag\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirements.txt (line 9)) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in e:\\genai\\samplerag\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirements.txt (line 9)) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in e:\\genai\\samplerag\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community->-r requirements.txt (line 11)) (3.4.4)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Using cached markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Collecting greenlet>=1\n",
      "  Using cached greenlet-3.2.4-cp311-cp311-win_amd64.whl (299 kB)\n",
      "Collecting networkx>=2.5.1\n",
      "  Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Collecting safetensors>=0.4.3\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
      "Collecting click>=8.0.0\n",
      "  Using cached click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Collecting shellingham>=1.3.0\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Collecting httptools>=0.6.3\n",
      "  Using cached httptools-0.7.1-cp311-cp311-win_amd64.whl (86 kB)\n",
      "Collecting watchfiles>=0.13\n",
      "  Using cached watchfiles-1.1.1-cp311-cp311-win_amd64.whl (287 kB)\n",
      "Collecting websockets>=10.4\n",
      "  Using cached websockets-15.0.1-cp311-cp311-win_amd64.whl (176 kB)\n",
      "Collecting widgetsnbextension~=4.0.14\n",
      "  Using cached widgetsnbextension-4.0.15-py3-none-any.whl (2.2 MB)\n",
      "Collecting jupyterlab_widgets~=3.0.15\n",
      "  Using cached jupyterlab_widgets-3.0.16-py3-none-any.whl (914 kB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Collecting argon2-cffi-bindings\n",
      "  Using cached argon2_cffi_bindings-25.1.0-cp39-abi3-win_amd64.whl (31 kB)\n",
      "Collecting webencodings\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting tinycss2<1.5,>=1.1.0\n",
      "  Using cached tinycss2-1.4.0-py3-none-any.whl (26 kB)\n",
      "Collecting wrapt<3,>=1.10\n",
      "  Using cached wrapt-2.0.1-cp311-cp311-win_amd64.whl (60 kB)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2\n",
      "  Using cached grpcio_status-1.76.0-py3-none-any.whl (14 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Using cached zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in e:\\genai\\samplerag\\lib\\site-packages (from jedi>=0.18.1->ipython>=7.23.1->ipykernel->-r requirements.txt (line 75)) (0.8.5)\n",
      "Collecting python-json-logger>=2.0.4\n",
      "  Using cached python_json_logger-4.0.0-py3-none-any.whl (15 kB)\n",
      "Collecting rfc3339-validator\n",
      "  Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
      "Collecting rfc3986-validator>=0.1.1\n",
      "  Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in e:\\genai\\samplerag\\lib\\site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain->-r requirements.txt (line 9)) (1.12.0)\n",
      "Collecting mdurl~=0.1\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting fastjsonschema>=2.15\n",
      "  Using cached fastjsonschema-2.21.2-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: wcwidth in e:\\genai\\samplerag\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel->-r requirements.txt (line 75)) (0.2.14)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Requirement already satisfied: executing>=1.2.0 in e:\\genai\\samplerag\\lib\\site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel->-r requirements.txt (line 75)) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in e:\\genai\\samplerag\\lib\\site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel->-r requirements.txt (line 75)) (3.0.1)\n",
      "Requirement already satisfied: pure-eval in e:\\genai\\samplerag\\lib\\site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel->-r requirements.txt (line 75)) (0.2.3)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Using cached mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Collecting humanfriendly>=9.1\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2\n",
      "  Using cached grpcio_status-1.75.1-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.75.0-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.74.0-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.73.1-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.73.0-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.72.2-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.72.1-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.71.2-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.71.0-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.70.0-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.69.0-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.68.1-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.68.0-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.67.1-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.67.0-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.66.2-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.66.1-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.66.0-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.65.5-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.65.4-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.65.2-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.65.1-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.64.3-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.64.1-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.64.0-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.63.2-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.63.0-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.62.3-py3-none-any.whl (14 kB)\n",
      "Collecting pyreadline3\n",
      "  Using cached pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
      "Collecting fqdn\n",
      "  Using cached fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Collecting isoduration\n",
      "  Using cached isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Collecting rfc3987-syntax>=1.1.0\n",
      "  Downloading rfc3987_syntax-1.1.0-py3-none-any.whl (8.0 kB)\n",
      "Collecting uri-template\n",
      "  Using cached uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
      "Collecting webcolors>=24.6.0\n",
      "  Using cached webcolors-25.10.0-py3-none-any.whl (14 kB)\n",
      "Collecting cffi>=1.0.1\n",
      "  Using cached cffi-2.0.0-cp311-cp311-win_amd64.whl (182 kB)\n",
      "Collecting pycparser\n",
      "  Using cached pycparser-2.23-py3-none-any.whl (118 kB)\n",
      "Collecting lark>=1.2.2\n",
      "  Downloading lark-1.3.1-py3-none-any.whl (113 kB)\n",
      "     -------------------------------------- 113.2/113.2 kB 6.4 MB/s eta 0:00:00\n",
      "Collecting arrow>=0.15.0\n",
      "  Using cached arrow-1.4.0-py3-none-any.whl (68 kB)\n",
      "Collecting tzdata\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: webencodings, pypika, mpmath, flatbuffers, fastjsonschema, durationpy, zipp, wrapt, widgetsnbextension, websockets, websocket-client, webcolors, urllib3, uri-template, tzdata, tqdm, tinycss2, threadpoolctl, sympy, soupsieve, shellingham, send2trash, safetensors, rpds-py, rfc3986-validator, rfc3339-validator, regex, pywinpty, python-json-logger, python-dotenv, pyreadline3, pyproject_hooks, pypdf, pycparser, pybase64, pyasn1, protobuf, propcache, prometheus-client, Pillow, pandocfilters, overrides, oauthlib, numpy, networkx, mypy-extensions, multidict, mmh3, mistune, mdurl, marshmallow, markupsafe, lxml, lark, jupyterlab_widgets, jupyterlab-pygments, json5, joblib, jiter, importlib-resources, httpx-sse, httptools, grpcio, greenlet, fsspec, frozenlist, fqdn, filelock, distro, defusedxml, click, cachetools, bleach, bcrypt, backoff, babel, attrs, async-lru, aiohappyeyeballs, yarl, watchfiles, uvicorn, typing-inspect, terminado, SQLAlchemy, scipy, rsa, rfc3987-syntax, referencing, pyasn1-modules, proto-plus, opentelemetry-proto, markdown-it-py, jinja2, importlib-metadata, humanfriendly, googleapis-common-protos, faiss-cpu, deprecated, cffi, build, beautifulsoup4, arrow, aiosignal, torch, tiktoken, scikit-learn, rich, requests-oauthlib, pydantic-settings, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, openai, jupyter-server-terminals, jsonschema-specifications, isoduration, ipywidgets, huggingface-hub, grpcio-status, google-auth, dataclasses-json, coloredlogs, argon2-cffi-bindings, aiohttp, typer, tokenizers, opentelemetry-semantic-conventions, onnxruntime, kubernetes, jupyter-console, jsonschema, google-api-core, argon2-cffi, transformers, opentelemetry-sdk, nbformat, sentence-transformers, opentelemetry-exporter-otlp-proto-grpc, nbclient, langchain-text-splitters, langchain-openai, langchain-huggingface, jupyter-events, google-ai-generativelanguage, nbconvert, langchain-classic, google-generativeai, chromadb, langchain-google-genai, langchain-community, langchain-chroma, jupyter-server, notebook-shim, jupyterlab-server, jupyter-lsp, jupyterlab, notebook, jupyter\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.5.0\n",
      "    Uninstalling urllib3-2.5.0:\n",
      "      Successfully uninstalled urllib3-2.5.0\n",
      "Successfully installed Pillow-12.0.0 SQLAlchemy-2.0.44 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 argon2-cffi-25.1.0 argon2-cffi-bindings-25.1.0 arrow-1.4.0 async-lru-2.0.5 attrs-25.4.0 babel-2.17.0 backoff-2.2.1 bcrypt-5.0.0 beautifulsoup4-4.14.2 bleach-6.3.0 build-1.3.0 cachetools-6.2.2 cffi-2.0.0 chromadb-1.3.4 click-8.3.1 coloredlogs-15.0.1 dataclasses-json-0.6.7 defusedxml-0.7.1 deprecated-1.3.1 distro-1.9.0 durationpy-0.10 faiss-cpu-1.13.0 fastjsonschema-2.21.2 filelock-3.20.0 flatbuffers-25.9.23 fqdn-1.5.1 frozenlist-1.8.0 fsspec-2025.10.0 google-ai-generativelanguage-0.4.0 google-api-core-2.28.1 google-auth-2.43.0 google-generativeai-0.3.2 googleapis-common-protos-1.72.0 greenlet-3.2.4 grpcio-1.76.0 grpcio-status-1.62.3 httptools-0.7.1 httpx-sse-0.4.3 huggingface-hub-0.36.0 humanfriendly-10.0 importlib-metadata-8.4.0 importlib-resources-6.5.2 ipywidgets-8.1.8 isoduration-20.11.0 jinja2-3.1.6 jiter-0.12.0 joblib-1.5.2 json5-0.12.1 jsonschema-4.25.1 jsonschema-specifications-2025.9.1 jupyter-1.1.1 jupyter-console-6.6.3 jupyter-events-0.12.0 jupyter-lsp-2.3.0 jupyter-server-2.17.0 jupyter-server-terminals-0.5.3 jupyterlab-4.4.10 jupyterlab-pygments-0.3.0 jupyterlab-server-2.28.0 jupyterlab_widgets-3.0.16 kubernetes-34.1.0 langchain-chroma-1.0.0 langchain-classic-1.0.0 langchain-community-0.4.1 langchain-google-genai-0.0.1 langchain-huggingface-1.0.1 langchain-openai-1.0.3 langchain-text-splitters-1.0.0 lark-1.3.1 lxml-6.0.2 markdown-it-py-4.0.0 markupsafe-3.0.3 marshmallow-3.26.1 mdurl-0.1.2 mistune-3.1.4 mmh3-5.2.0 mpmath-1.3.0 multidict-6.7.0 mypy-extensions-1.1.0 nbclient-0.10.2 nbconvert-7.16.6 nbformat-5.10.4 networkx-3.5 notebook-7.4.7 notebook-shim-0.2.4 numpy-2.3.5 oauthlib-3.3.1 onnxruntime-1.23.2 openai-2.8.1 opentelemetry-api-1.27.0 opentelemetry-exporter-otlp-proto-common-1.27.0 opentelemetry-exporter-otlp-proto-grpc-1.27.0 opentelemetry-proto-1.27.0 opentelemetry-sdk-1.27.0 opentelemetry-semantic-conventions-0.48b0 overrides-7.7.0 pandocfilters-1.5.1 posthog-5.4.0 prometheus-client-0.23.1 propcache-0.4.1 proto-plus-1.26.1 protobuf-4.25.8 pyasn1-0.6.1 pyasn1-modules-0.4.2 pybase64-1.4.2 pycparser-2.23 pydantic-settings-2.12.0 pypdf-6.3.0 pypika-0.48.9 pyproject_hooks-1.2.0 pyreadline3-3.5.4 python-dotenv-1.2.1 python-json-logger-4.0.0 pywinpty-3.0.2 referencing-0.37.0 regex-2025.11.3 requests-oauthlib-2.0.0 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 rfc3987-syntax-1.1.0 rich-14.2.0 rpds-py-0.29.0 rsa-4.9.1 safetensors-0.6.2 scikit-learn-1.7.2 scipy-1.16.3 send2trash-1.8.3 sentence-transformers-5.1.2 shellingham-1.5.4 soupsieve-2.8 sympy-1.14.0 terminado-0.18.1 threadpoolctl-3.6.0 tiktoken-0.12.0 tinycss2-1.4.0 tokenizers-0.22.1 torch-2.9.1 tqdm-4.67.1 transformers-4.57.1 typer-0.20.0 typing-inspect-0.9.0 tzdata-2025.2 uri-template-1.3.0 urllib3-2.3.0 uvicorn-0.38.0 watchfiles-1.1.1 webcolors-25.10.0 webencodings-0.5.1 websocket-client-1.9.0 websockets-15.0.1 widgetsnbextension-4.0.15 wrapt-2.0.1 yarl-1.22.0 zipp-3.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40344d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.5'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "from langchain_core import __version__ as core_version\n",
    "\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a768888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.5'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0633832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " langchain 1.0 detected\n"
     ]
    }
   ],
   "source": [
    "if langchain.__version__>=\"1.0\":\n",
    "    print(\" langchain 1.0 detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a282d2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " open api key present\n",
      "AIzaSyA-u4vjoBFfJZq_83J2loSJTjSPtid0KWs\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "if os.getenv(\"GOOGLE_API_KEY\"):\n",
    "    print(\" open api key present\")\n",
    "else:\n",
    "    print(\"open api key is not present\")\n",
    "print(os.getenv(\"GOOGLE_API_KEY\"))\n",
    "gemini_key=os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62e0256d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content:\n",
      "LangChain makes building LLM applications easy!\n",
      "\n",
      "Metadata:\n",
      "{'source': 'introduction.txt', 'author': 'LangChain Team', 'date': '2025-01-15'}\n",
      "\n",
      "Source: introduction.txt\n",
      "author information LangChain Team\n"
     ]
    }
   ],
   "source": [
    "# Creating a Document manually\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Create a simple document\n",
    "doc = Document(\n",
    "    page_content=\"LangChain makes building LLM applications easy!\",\n",
    "    metadata={\n",
    "        \"source\": \"introduction.txt\",\n",
    "        \"author\": \"LangChain Team\",\n",
    "        \"date\": \"2025-01-15\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Access the content\n",
    "print(\"Content:\")\n",
    "print(doc.page_content)\n",
    "print(\"\\nMetadata:\")\n",
    "print(doc.metadata)\n",
    "print(f\"\\nSource: {doc.metadata['source']}\")\n",
    "print(f\"author information {doc.metadata['author']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23083e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=[\n",
    "    Document(\n",
    "        page_content=\" This is the document content.\",\n",
    "        metadata={\"category 1\":\"programming 1\"}\n",
    "    ),\n",
    "     Document(\n",
    "        page_content=\" This is the document content 2.\",\n",
    "        metadata={\"category 2\":\"programming 2\"}\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c9f28fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content=' This is the document content.' metadata={'category 1': 'programming 1'}\n",
      "page_content=' This is the document content 2.' metadata={'category 2': 'programming 2'}\n"
     ]
    }
   ],
   "source": [
    "for i in documents:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2f9c302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'hello langchain'\n",
      "\n",
      "Processing:\n",
      "  Step 1: uppercase  HELLO LANGCHAIN\n",
      "  Step 2: add_prefix  RESULT: HELLO LANGCHAIN\n",
      "  Step 3: add_emoji   RESULT: HELLO LANGCHAIN\n",
      "\n",
      "Final Output:  RESULT: HELLO LANGCHAIN\n"
     ]
    }
   ],
   "source": [
    "# Simple LCEL Example: String Transformation Chain\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Create simple transformation functions\n",
    "def uppercase(text: str) -> str:\n",
    "    \"\"\"Convert text to uppercase\"\"\"\n",
    "    print(f\"  Step 1: uppercase  {text.upper()}\")\n",
    "    return text.upper()\n",
    "\n",
    "def add_prefix(text: str) -> str:\n",
    "    \"\"\"Add a prefix to text\"\"\"\n",
    "    result = f\"RESULT: {text}\"\n",
    "    print(f\"  Step 2: add_prefix  {result}\")\n",
    "    return result\n",
    "\n",
    "def add_emoji(text: str) -> str:\n",
    "    \"\"\"Add emoji to text\"\"\"\n",
    "    result = f\" {text}\"\n",
    "    print(f\"  Step 3: add_emoji  {result}\")\n",
    "    return result\n",
    "\n",
    "# Create runnables (components that can be chained)\n",
    "uppercase_runnable = RunnableLambda(uppercase)\n",
    "prefix_runnable = RunnableLambda(add_prefix)\n",
    "emoji_runnable = RunnableLambda(add_emoji)\n",
    "\n",
    "# Build the chain using LCEL\n",
    "chain = uppercase_runnable | prefix_runnable | emoji_runnable\n",
    "\n",
    "# Execute the chain\n",
    "print(\"Input: 'hello langchain'\")\n",
    "print(\"\\nProcessing:\")\n",
    "result = chain.invoke(\"hello langchain\")\n",
    "print(f\"\\nFinal Output: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3bf223a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" model='gpt-3.5-turbo'\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(\n",
    "    model='gpt-4.1-nano',\n",
    "    temperature=0\n",
    ")\n",
    " \n",
    "\" model='gpt-3.5-turbo'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7a2ae50",
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: your_ope************here. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m response=\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwhat is LangChain in one sentence\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:385\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    371\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    373\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    378\u001b[39m     **kwargs: Any,\n\u001b[32m    379\u001b[39m ) -> AIMessage:\n\u001b[32m    380\u001b[39m     config = ensure_config(config)\n\u001b[32m    381\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    382\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    383\u001b[39m         cast(\n\u001b[32m    384\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    395\u001b[39m         ).message,\n\u001b[32m    396\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1104\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1095\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1097\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1101\u001b[39m     **kwargs: Any,\n\u001b[32m   1102\u001b[39m ) -> LLMResult:\n\u001b[32m   1103\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1104\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:914\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    911\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    912\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    913\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m         )\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    922\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1208\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1206\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1207\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1208\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1212\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1333\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1331\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mhttp_response\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1332\u001b[39m         e.response = raw_response.http_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1333\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1334\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1335\u001b[39m     \u001b[38;5;28mself\u001b[39m.include_response_headers\n\u001b[32m   1336\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1337\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1338\u001b[39m ):\n\u001b[32m   1339\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1328\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1321\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[32m   1322\u001b[39m             response,\n\u001b[32m   1323\u001b[39m             schema=original_schema_obj,\n\u001b[32m   1324\u001b[39m             metadata=generation_info,\n\u001b[32m   1325\u001b[39m             output_version=\u001b[38;5;28mself\u001b[39m.output_version,\n\u001b[32m   1326\u001b[39m         )\n\u001b[32m   1327\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1328\u001b[39m         raw_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1329\u001b[39m         response = raw_response.parse()\n\u001b[32m   1330\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\openai\\_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\openai\\_utils\\_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1189\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1142\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1144\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1186\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1187\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1188\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1189\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAuthenticationError\u001b[39m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: your_ope************here. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "response=llm.invoke(\"what is LangChain in one sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c03781ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_core.pydantic_v1'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_google_genai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatGoogleGenerativeAI\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\langchain_google_genai\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_google_genai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatGoogleGenerativeAI\n\u001b[32m      3\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mChatGoogleGenerativeAI\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:42\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmessages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     33\u001b[39m     AIMessage,\n\u001b[32m     34\u001b[39m     AIMessageChunk,\n\u001b[32m   (...)\u001b[39m\u001b[32m     39\u001b[39m     HumanMessageChunk,\n\u001b[32m     40\u001b[39m )\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moutputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatGeneration, ChatGenerationChunk, ChatResult\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpydantic_v1\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Field, root_validator\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_from_dict_or_env\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtenacity\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     45\u001b[39m     before_sleep_log,\n\u001b[32m     46\u001b[39m     retry,\n\u001b[32m   (...)\u001b[39m\u001b[32m     49\u001b[39m     wait_exponential,\n\u001b[32m     50\u001b[39m )\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_core.pydantic_v1'"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4d48fda",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2174557176.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpip install -U langchain-google-genai\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Google Gemini (optional - for Notebook 04)\n",
    "pip install -U langchain-google-genai\n",
    "# google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa37933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "820d3461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-google-genai in e:\\genai\\samplerag\\lib\\site-packages (0.0.1)\n",
      "Collecting langchain-google-genai\n",
      "  Using cached langchain_google_genai-3.0.3-py3-none-any.whl (56 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Collecting google-ai-generativelanguage<1.0.0,>=0.7.0\n",
      "  Using cached google_ai_generativelanguage-0.9.0-py3-none-any.whl (1.4 MB)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in e:\\genai\\samplerag\\lib\\site-packages (from langchain-google-genai) (1.0.5)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in e:\\genai\\samplerag\\lib\\site-packages (from langchain-google-genai) (2.12.4)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in e:\\genai\\samplerag\\lib\\site-packages (from google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (2.28.1)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in e:\\genai\\samplerag\\lib\\site-packages (from google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (2.43.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in e:\\genai\\samplerag\\lib\\site-packages (from google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (1.76.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in e:\\genai\\samplerag\\lib\\site-packages (from google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (1.26.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in e:\\genai\\samplerag\\lib\\site-packages (from google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (4.25.8)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in e:\\genai\\samplerag\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in e:\\genai\\samplerag\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (0.4.43)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in e:\\genai\\samplerag\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in e:\\genai\\samplerag\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in e:\\genai\\samplerag\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in e:\\genai\\samplerag\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (4.15.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in e:\\genai\\samplerag\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in e:\\genai\\samplerag\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in e:\\genai\\samplerag\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (0.4.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in e:\\genai\\samplerag\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (1.72.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in e:\\genai\\samplerag\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (2.32.5)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in e:\\genai\\samplerag\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (1.62.3)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in e:\\genai\\samplerag\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (6.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in e:\\genai\\samplerag\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in e:\\genai\\samplerag\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (4.9.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in e:\\genai\\samplerag\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in e:\\genai\\samplerag\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in e:\\genai\\samplerag\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in e:\\genai\\samplerag\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in e:\\genai\\samplerag\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (0.25.0)\n",
      "Requirement already satisfied: anyio in e:\\genai\\samplerag\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (4.11.0)\n",
      "Requirement already satisfied: certifi in e:\\genai\\samplerag\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in e:\\genai\\samplerag\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (1.0.9)\n",
      "Requirement already satisfied: idna in e:\\genai\\samplerag\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in e:\\genai\\samplerag\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (0.16.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in e:\\genai\\samplerag\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (0.6.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in e:\\genai\\samplerag\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\genai\\samplerag\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (2.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in e:\\genai\\samplerag\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (1.3.1)\n",
      "Installing collected packages: filetype, google-ai-generativelanguage, langchain-google-genai\n",
      "  Attempting uninstall: google-ai-generativelanguage\n",
      "    Found existing installation: google-ai-generativelanguage 0.4.0\n",
      "    Uninstalling google-ai-generativelanguage-0.4.0:\n",
      "      Successfully uninstalled google-ai-generativelanguage-0.4.0\n",
      "  Attempting uninstall: langchain-google-genai\n",
      "    Found existing installation: langchain-google-genai 0.0.1\n",
      "    Uninstalling langchain-google-genai-0.0.1:\n",
      "      Successfully uninstalled langchain-google-genai-0.0.1\n",
      "Successfully installed filetype-1.2.0 google-ai-generativelanguage-0.9.0 langchain-google-genai-3.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-generativeai 0.3.2 requires google-ai-generativelanguage==0.4.0, but you have google-ai-generativelanguage 0.9.0 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59437ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is the code for Gemini Embeddings:\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# Test the embeddings with a sample text\n",
    "sample_text = \"This is a test sentence to demonstrate embeddings.\"\n",
    "# sample_embedding = embeddings.embed_query(sample_text)\n",
    "\n",
    "print(f\" Embeddings model initialized: text-embedding-3-small\")\n",
    "print(f\" Embedding dimension: {len(sample_embedding)}\")\n",
    "print(f\" Sample embedding (first 10 values): {sample_embedding[:10]}\")\n",
    "print(f\" Each chunk will be converted to a {len(sample_embedding)}-dimensional vector for similarity search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fcfe7aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm=ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0,\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "676993f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -q -U google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8b5a44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI learns patterns from data to make intelligent decisions.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "# The client gets the API key from the environment variable `GEMINI_API_KEY`.\n",
    "client = genai.Client(api_key=gemini_key)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=\"Explain how AI works in a few words\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4f00192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi there! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--99623f05-81bd-4a44-81ea-9b140f39eb39-0', usage_metadata={'input_tokens': 2, 'output_tokens': 184, 'total_tokens': 186, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 174}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0,\n",
    "    api_key=gemini_key)\n",
    "llm.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95822d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LangChain is a framework for developing applications powered by large language models, enabling them to connect with external data and computation through modular components.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--83b47982-e402-40ca-87d9-25bbbaf632ab-0', usage_metadata={'input_tokens': 7, 'output_tokens': 751, 'total_tokens': 758, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 724}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=llm.invoke(\"what is langchain in one sentence\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45fb7ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a framework for developing applications powered by large language models, enabling them to connect with external data and computation through modular components.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "80b373a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt template\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt=ChatPromptTemplate.from_template\n",
    "(\n",
    "    \"explain the {topic} in detail\"\n",
    ")\n",
    "print(\"prompt template\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6b84a530",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, got dict",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m explanation = \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtopic\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43martificial intelligence\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3127\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3125\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3126\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3127\u001b[39m         input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3128\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3129\u001b[39m         input_ = context.run(step.invoke, input_, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4857\u001b[39m, in \u001b[36mRunnableLambda.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   4842\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Invoke this `Runnable` synchronously.\u001b[39;00m\n\u001b[32m   4843\u001b[39m \n\u001b[32m   4844\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4854\u001b[39m \n\u001b[32m   4855\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4856\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfunc\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m4857\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4858\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4859\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4860\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4861\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4862\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4863\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mCannot invoke a coroutine function synchronously.Use `ainvoke` instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4864\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2050\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   2046\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   2047\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   2048\u001b[39m         output = cast(\n\u001b[32m   2049\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m2050\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2051\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   2052\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2053\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2054\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2055\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2056\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2057\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   2058\u001b[39m         )\n\u001b[32m   2059\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2060\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\langchain_core\\runnables\\config.py:428\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    427\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4714\u001b[39m, in \u001b[36mRunnableLambda._invoke\u001b[39m\u001b[34m(self, input_, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m   4712\u001b[39m                 output = chunk\n\u001b[32m   4713\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4714\u001b[39m     output = \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4715\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   4716\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4717\u001b[39m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[32m   4718\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\langchain_core\\runnables\\config.py:428\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    427\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\langchain_core\\prompts\\chat.py:1091\u001b[39m, in \u001b[36mChatPromptTemplate.from_template\u001b[39m\u001b[34m(cls, template, **kwargs)\u001b[39m\n\u001b[32m   1077\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   1078\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_template\u001b[39m(\u001b[38;5;28mcls\u001b[39m, template: \u001b[38;5;28mstr\u001b[39m, **kwargs: Any) -> ChatPromptTemplate:\n\u001b[32m   1079\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create a chat prompt template from a template string.\u001b[39;00m\n\u001b[32m   1080\u001b[39m \n\u001b[32m   1081\u001b[39m \u001b[33;03m    Creates a chat template consisting of a single message assumed to be from\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1089\u001b[39m \u001b[33;03m        A new instance of this class.\u001b[39;00m\n\u001b[32m   1090\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m     prompt_template = \u001b[43mPromptTemplate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1092\u001b[39m     message = HumanMessagePromptTemplate(prompt=prompt_template)\n\u001b[32m   1093\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.from_messages([message])\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\langchain_core\\prompts\\prompt.py:289\u001b[39m, in \u001b[36mPromptTemplate.from_template\u001b[39m\u001b[34m(cls, template, template_format, partial_variables, **kwargs)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_template\u001b[39m(\n\u001b[32m    253\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    258\u001b[39m     **kwargs: Any,\n\u001b[32m    259\u001b[39m ) -> PromptTemplate:\n\u001b[32m    260\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load a prompt template from a template.\u001b[39;00m\n\u001b[32m    261\u001b[39m \n\u001b[32m    262\u001b[39m \u001b[33;03m    *Security warning*:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    287\u001b[39m \u001b[33;03m        The prompt template loaded from the template.\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m     input_variables = \u001b[43mget_template_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    290\u001b[39m     partial_variables_ = partial_variables \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m    292\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m partial_variables_:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\langchain_core\\prompts\\string.py:268\u001b[39m, in \u001b[36mget_template_variables\u001b[39m\u001b[34m(template, template_format)\u001b[39m\n\u001b[32m    265\u001b[39m     input_variables = _get_jinja2_variables_from_template(template)\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m template_format == \u001b[33m\"\u001b[39m\u001b[33mf-string\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    267\u001b[39m     input_variables = {\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m         v \u001b[38;5;28;01mfor\u001b[39;00m _, v, _, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[43mFormatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    269\u001b[39m     }\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m template_format == \u001b[33m\"\u001b[39m\u001b[33mmustache\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    271\u001b[39m     input_variables = mustache_template_vars(template)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\string.py:288\u001b[39m, in \u001b[36mFormatter.parse\u001b[39m\u001b[34m(self, format_string)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_string):\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _string.formatter_parser(format_string)\n",
      "\u001b[31mTypeError\u001b[39m: expected str, got dict"
     ]
    }
   ],
   "source": [
    "explanation = chain.invoke({\"topic\": \"artificial intelligence\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6562c8aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'messages'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mprompt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessages\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mAttributeError\u001b[39m: 'function' object has no attribute 'messages'"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Correctly call the from_template function with the template string\n",
    "prompt = ChatPromptTemplate.from_template(\"explain the {topic} in detail\")\n",
    "\n",
    "# Access the first message in the prompt\n",
    "print(\"prompt template\")\n",
    "print(prompt.messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65818d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Topic: MACHINE LEARNING\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "topics = [\"machine learning\"]\n",
    "for topic in topics:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Topic: {topic.upper()}\")\n",
    "    print('='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c98ffda0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'machine learning'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fe3f5d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Topic: MACHINE LEARNING\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected mapping type as input to ChatPromptTemplate. Received <class 'str'>.\nFor troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/INVALID_PROMPT_INPUT ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Invoke the chain\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m explanation = \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmachine learning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(explanation)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3127\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3125\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3126\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3127\u001b[39m         input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3128\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3129\u001b[39m         input_ = context.run(step.invoke, input_, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4857\u001b[39m, in \u001b[36mRunnableLambda.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   4842\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Invoke this `Runnable` synchronously.\u001b[39;00m\n\u001b[32m   4843\u001b[39m \n\u001b[32m   4844\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4854\u001b[39m \n\u001b[32m   4855\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4856\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfunc\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m4857\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4858\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4859\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4860\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4861\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4862\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4863\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mCannot invoke a coroutine function synchronously.Use `ainvoke` instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4864\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2050\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   2046\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   2047\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   2048\u001b[39m         output = cast(\n\u001b[32m   2049\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m2050\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2051\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   2052\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2053\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2054\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2055\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2056\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2057\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   2058\u001b[39m         )\n\u001b[32m   2059\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2060\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\langchain_core\\runnables\\config.py:428\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    427\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4725\u001b[39m, in \u001b[36mRunnableLambda._invoke\u001b[39m\u001b[34m(self, input_, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m   4721\u001b[39m         msg = (\n\u001b[32m   4722\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRecursion limit reached when invoking \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4723\u001b[39m         )\n\u001b[32m   4724\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRecursionError\u001b[39;00m(msg)\n\u001b[32m-> \u001b[39m\u001b[32m4725\u001b[39m     output = \u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4726\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4727\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4728\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4729\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4730\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrecursion_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrecursion_limit\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4731\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4732\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4733\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\langchain_core\\prompts\\base.py:217\u001b[39m, in \u001b[36mBasePromptTemplate.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tags:\n\u001b[32m    216\u001b[39m     config[\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m] += \u001b[38;5;28mself\u001b[39m.tags\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_format_prompt_with_error_handling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserialized\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_serialized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2050\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   2046\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   2047\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   2048\u001b[39m         output = cast(\n\u001b[32m   2049\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m2050\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2051\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   2052\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2053\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2054\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2055\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2056\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2057\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   2058\u001b[39m         )\n\u001b[32m   2059\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2060\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\langchain_core\\runnables\\config.py:428\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    427\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\langchain_core\\prompts\\base.py:190\u001b[39m, in \u001b[36mBasePromptTemplate._format_prompt_with_error_handling\u001b[39m\u001b[34m(self, inner_input)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_format_prompt_with_error_handling\u001b[39m(\u001b[38;5;28mself\u001b[39m, inner_input: \u001b[38;5;28mdict\u001b[39m) -> PromptValue:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     inner_input_ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_prompt(**inner_input_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\langchain_core\\prompts\\base.py:166\u001b[39m, in \u001b[36mBasePromptTemplate._validate_input\u001b[39m\u001b[34m(self, inner_input)\u001b[39m\n\u001b[32m    161\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    162\u001b[39m         msg = (\n\u001b[32m    163\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected mapping type as input to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    164\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReceived \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(inner_input)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    165\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    167\u001b[39m             create_message(\n\u001b[32m    168\u001b[39m                 message=msg, error_code=ErrorCode.INVALID_PROMPT_INPUT\n\u001b[32m    169\u001b[39m             )\n\u001b[32m    170\u001b[39m         )\n\u001b[32m    171\u001b[39m missing = \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m.input_variables).difference(inner_input)\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m missing:\n",
      "\u001b[31mTypeError\u001b[39m: Expected mapping type as input to ChatPromptTemplate. Received <class 'str'>.\nFor troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/INVALID_PROMPT_INPUT "
     ]
    }
   ],
   "source": [
    "# Build a chain: Prompt  LLM\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Components:\n",
    "# 1. prompt: Formats the input\n",
    "# 2. llm: Generates the response\n",
    "# 3. StrOutputParser: Extracts just the text from the response\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Use the chain with different topics\n",
    "topics = [\"machine learning\"]\n",
    "\n",
    "for topic in topics:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Topic: {topic.upper()}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Invoke the chain\n",
    "    explanation = chain.invoke(\"machine learning\")\n",
    "    print(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4ac2f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='**mL** stands for **milliliter**.\\n\\nIt is a unit of **volume** in the metric system.\\n\\nHere\\'s a breakdown:\\n\\n*   **Milli-**: This prefix means one-thousandth (1/1000).\\n*   **Liter (L)**: This is the base unit of volume in the metric system.\\n\\nSo, **one milliliter (1 mL) is one-thousandth of a liter (0.001 L)**.\\n\\n**Key points about mL:**\\n\\n*   **Conversion:**\\n    *   1 L = 1000 mL\\n    *   1 mL = 0.001 L\\n*   **Equivalence to Cubic Centimeter:** 1 mL is exactly equal to 1 cubic centimeter (cm). This means a cube with sides of 1 cm each would hold exactly 1 mL of liquid.\\n*   **Common Uses:** You\\'ll frequently encounter mL when:\\n    *   Measuring liquids in cooking and baking (e.g., recipes).\\n    *   Dosing medication (e.g., liquid cough syrup, infant formula).\\n    *   Conducting scientific experiments and laboratory work.\\n    *   Labeling beverages (e.g., a can of soda might be 330 mL).\\n    *   Measuring small amounts of chemicals or reagents.\\n\\nIn short, if you see \"mL\", think \"a small amount of liquid volume.\"', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--0eda38e1-3366-4f88-9864-a717f15db61f-0', usage_metadata={'input_tokens': 4, 'output_tokens': 1152, 'total_tokens': 1156, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 846}})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"what is mL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c327d66c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b484b9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Topic: MACHINE LEARNING\n",
      "============================================================\n",
      "A VectorDB stores and indexes high-dimensional vector embeddings, enabling rapid similarity searches based on semantic meaning rather than exact matches.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Define the prompt template\n",
    "#prompt = ChatPromptTemplate.from_template(\"Explain the {topic} in one statement\")\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(\n",
    "    \"explain the {topic} in one sentence\"\n",
    "    )\n",
    "\n",
    "# Define the chain\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Use the chain with different topics\n",
    "topics = [\"machine learning\"]\n",
    "\n",
    "for topic in topics:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Topic: {topic.upper()}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Invoke the chain with a dictionary containing the topic\n",
    "    explanation = chain.invoke({\"topic\": \"vectorDB\"})\n",
    "    print(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "154f7de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "802c9652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.5'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "601a28e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.5'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain_core\n",
    "langchain_core.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b7fe8925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.1'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain_openai\n",
    "import langchain_community\n",
    "langchain_community.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4ee98f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings,ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage,AIMessage\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1215225a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=PyPDFLoader(\"attention.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ddc28921",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dc2f6a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez \n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "ukasz Kaiser\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin \n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring significantly\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
      "to-German translation task, improving over the existing best results, including\n",
      "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
      "our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
      "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
      "best models from the literature. We show that the Transformer generalizes well to\n",
      "other tasks by applying it successfully to English constituency parsing both with\n",
      "large and limited training data.\n",
      "Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\n",
      "has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
      "attention and the parameter-free position representation and became the other person involved in nearly every\n",
      "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
      "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\n",
      "efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n",
      "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\n",
      "our research.\n",
      "Work performed while at Google Brain.\n",
      "Work performed while at Google Research.\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\n",
      "arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\n"
     ]
    }
   ],
   "source": [
    "print(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0935fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TExt splitting... \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1b20ad63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI Embeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",  # Latest, cost-effective embedding model\n",
    "    # Alternative: \"text-embedding-3-large\" for better quality\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6091e1f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "APIConnectionError",
     "evalue": "Connection error.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\httpx\\_transports\\default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\httpcore\\_sync\\connection.py:101\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\httpcore\\_sync\\connection.py:78\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m     ssl_object = stream.get_extra_info(\u001b[33m\"\u001b[39m\u001b[33mssl_object\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\httpcore\\_sync\\connection.py:156\u001b[39m, in \u001b[36mHTTPConnection._connect\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mstart_tls\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     stream = \u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart_tls\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m     trace.return_value = stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\httpcore\\_backends\\sync.py:154\u001b[39m, in \u001b[36mSyncStream.start_tls\u001b[39m\u001b[34m(self, ssl_context, server_hostname, timeout)\u001b[39m\n\u001b[32m    150\u001b[39m exc_map: ExceptionMapping = {\n\u001b[32m    151\u001b[39m     socket.timeout: ConnectTimeout,\n\u001b[32m    152\u001b[39m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[32m    153\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py:155\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m     \u001b[38;5;28mself\u001b[39m.gen.throw(typ, value, traceback)\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    157\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    158\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    159\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\httpcore\\_exceptions.py:14\u001b[39m, in \u001b[36mmap_exceptions\u001b[39m\u001b[34m(map)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:992)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\openai\\_base_client.py:982\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\httpx\\_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\httpx\\_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\httpx\\_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\httpx\\_transports\\default.py:249\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_httpcore_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py:155\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m     \u001b[38;5;28mself\u001b[39m.gen.throw(typ, value, traceback)\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    157\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    158\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    159\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\httpx\\_transports\\default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:992)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mAPIConnectionError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m sample_text=\u001b[33m\"\u001b[39m\u001b[33mThis is the test to check the sample embedding technique.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m sample_embedding=\u001b[43membeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:752\u001b[39m, in \u001b[36mOpenAIEmbeddings.embed_query\u001b[39m\u001b[34m(self, text, **kwargs)\u001b[39m\n\u001b[32m    742\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Call out to OpenAI's embedding endpoint for embedding query text.\u001b[39;00m\n\u001b[32m    743\u001b[39m \n\u001b[32m    744\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    749\u001b[39m \u001b[33;03m    Embedding for the text.\u001b[39;00m\n\u001b[32m    750\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    751\u001b[39m \u001b[38;5;28mself\u001b[39m._ensure_sync_client_available()\n\u001b[32m--> \u001b[39m\u001b[32m752\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:702\u001b[39m, in \u001b[36mOpenAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    699\u001b[39m \u001b[38;5;66;03m# Unconditionally call _get_len_safe_embeddings to handle length safety.\u001b[39;00m\n\u001b[32m    700\u001b[39m \u001b[38;5;66;03m# This could be optimized to avoid double work when all texts are short enough.\u001b[39;00m\n\u001b[32m    701\u001b[39m engine = cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m.deployment)\n\u001b[32m--> \u001b[39m\u001b[32m702\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:569\u001b[39m, in \u001b[36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[39m\u001b[34m(self, texts, engine, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    567\u001b[39m \u001b[38;5;66;03m# Make API call with this batch\u001b[39;00m\n\u001b[32m    568\u001b[39m batch_tokens = tokens[i:batch_end]\n\u001b[32m--> \u001b[39m\u001b[32m569\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mbatch_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    571\u001b[39m     response = response.model_dump()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\openai\\resources\\embeddings.py:132\u001b[39m, in \u001b[36mEmbeddings.create\u001b[39m\u001b[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m             embedding.embedding = np.frombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[32m    127\u001b[39m                 base64.b64decode(data), dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m             ).tolist()\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GENAI\\samplerag\\Lib\\site-packages\\openai\\_base_client.py:1014\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1011\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1013\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRaising connection error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request=request) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   1016\u001b[39m log.debug(\n\u001b[32m   1017\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mHTTP Response: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m,\n\u001b[32m   1018\u001b[39m     request.method,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     response.headers,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mrequest_id: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, response.headers.get(\u001b[33m\"\u001b[39m\u001b[33mx-request-id\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mAPIConnectionError\u001b[39m: Connection error."
     ]
    }
   ],
   "source": [
    "sample_text=\"This is the test to check the sample embedding technique.\"\n",
    "sample_embedding=embeddings.embed_query(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c69a7521",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from the environment variable\n",
    "gemini_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Initialize the embeddings model\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"gemini-embedding-001\",  # Specify the Gemini embedding model\n",
    "    api_key=gemini_api_key         # Pass the API key\n",
    ")\n",
    "\n",
    "# Define the text to embed\n",
    "sample_text = \"This is a test sentence to demonstrate embeddings.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5054c248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings\n",
    "try:\n",
    "    sample_embedding = embeddings.embed_query(sample_text)\n",
    "    print(\"Embedding generated successfully!\")\n",
    "    print(f\"Embedding length: {len(sample_embedding)}\")\n",
    "    print(f\"First 10 dimensions: {sample_embedding[:10]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error generating embedding: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e8923472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3072\n"
     ]
    }
   ],
   "source": [
    "sample_embedding=embeddings.embed_query(sample_text)\n",
    "print(len(sample_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20366fec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samplerag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
